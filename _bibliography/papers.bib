---
---
@inproceedings{hughes-etal-2025-responsivity,
    title = "Computational Analysis of Conversation Dynamics through Participant Responsivity",
    author = "Hughes, Margaret  and
      Roy, Brandon  and
      Poole-Dayan, Elinor  and
      Roy, Deb  and
      Kabbara, Jad",
    booktitle = "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    abbr={EMNLP 2025},
    selected={true},
}

@mastersthesis{poole-dayan2025dialogue,
  author       = {Poole-Dayan, Elinor},
  title        = {From Dialogue to Decision: An LLM-Powered Framework for Analyzing Collective Idea Evolution and Voting Dynamics in Deliberative Assemblies},
  school       = {Massachusetts Institute of Technology},
  year         = {2025},
  type         = {Master's thesis},
  address      = {Cambridge, MA},
  note         = {Advisor: Deb Roy},
  abbr         = {MIT 2025},
  url          = {https://www.media.mit.edu/publications/from-dialogue-to-decision-an-llm-powered-framework-deliberative-assemblies/},
  pdf          = {https://www.media.mit.edu/publications/from-dialogue-to-decision-an-llm-powered-framework-deliberative-assemblies/}
}
@misc{pooledayan2024llmtargetedunderperformancedisproportionately,
      title={LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users}, 
      author={Elinor Poole-Dayan and Deb Roy and Jad Kabbara},
      year={2024},
      eprint={2406.17737},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17737}, 
      abbr={NeurIPS 2024},
      selected={true},
      arxiv={2406.17737},
}
@inproceedings{fulay-etal-2024-relationship,
    title = "On the Relationship between Truth and Political Bias in Language Models",
    author = "Fulay, Suyash  and
      Brannon, William  and
      Mohanty, Shrestha  and
      Overney, Cassandra  and
      Poole-Dayan, Elinor  and
      Roy, Deb  and
      Kabbara, Jad",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.508",
    pdf = "https://aclanthology.org/2024.emnlp-main.508.pdf",
    pages = "9004--9018",
    abstract = "Language model alignment research often attempts to ensure that models are not only helpful and harmless, but also truthful and unbiased. However, optimizing these objectives simultaneously can obscure how improving one aspect might impact the others. In this work, we focus on analyzing the relationship between two concepts essential in both language model alignment and political science: truthfulness and political bias. We train reward models on various popular truthfulness datasets and subsequently evaluate their political bias. Our findings reveal that optimizing reward models for truthfulness on these datasets tends to result in a left-leaning political bias. We also find that existing open-source reward models (i.e., those trained on standard human preference datasets) already show a similar bias and that the bias is larger for larger models. These results raise important questions about the datasets used to represent truthfulness, potential limitations of aligning models to be both truthful and politically unbiased, and what language models capture about the relationship between truth and politics.",
    abbr={EMNLP 2024},
    selected={true},
    arxiv={2409.05283},
}

@inproceedings{meade-etal-2022-empirical,
    title = "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
    author = "Meade, Nicholas  and
      Poole-Dayan, Elinor  and
      Reddy, Siva",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.132",
    pdf = "https://aclanthology.org/2022.acl-long.132.pdf",
    doi = "10.18653/v1/2022.acl-long.132",
    pages = "1878--1898",
    abstract = "Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on. This has attracted attention to developing techniques that mitigate such biases. In this work, we perform an empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model{'}s language modeling ability, as well as its performance on downstream NLU tasks. We experimentally find that: (1) Self-Debias is the strongest debiasing technique, obtaining improved scores on all bias benchmarks; (2) Current debiasing techniques perform less consistently when mitigating non-gender biases; And (3) improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability, making it difficult to determine whether the bias mitigation was effective.",
    abbr={ACL 2022},
    arxiv={2110.08527},
    selected={true},
}

@inproceedings{NEURIPS2023_1a675d80,
 author = {Krojer, Benno and Poole-Dayan, Elinor and Voleti, Vikram and Pal, Chris and Reddy, Siva},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {8385--8405},
 publisher = {Curran Associates, Inc.},
 title = {Are Diffusion Models Vision-And-Language Reasoners?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/1a675d804f50509b8e21d0d3ca709d03-Paper-Conference.pdf},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/1a675d804f50509b8e21d0d3ca709d03-Paper-Conference.pdf},
 volume = {36},
 year = {2023},
 abstract = {Text-conditioned image generation models have recently shown immense qualitative success using denoising diffusion processes. However, unlike discriminative vision-and-language models, it is a non-trivial task to subject these diffusion-based generative models to automatic fine-grained quantitative evaluation of high-level phenomena such as compositionality.Towards this goal, we perform two innovations. First, we transform diffusion-based models (in our case, Stable Diffusion) for any image-text matching (ITM) task using a novel method called DiffusionITM.Second, we introduce the Generative-Discriminative Evaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language tasks, bias evaluation and detailed analysis.We find that Stable Diffusion + DiffusionITM is competitive on many tasks and outperforms CLIP on compositional tasks like like CLEVR and Winoground.We further boost its compositional performance with a transfer setup by fine-tuning on MS-COCO while retaining generative capabilities. We also measure the stereotypical bias in diffusion models, and find that Stable Diffusion 2.1 is, for the most part, less biased than Stable Diffusion 1.5.Overall, our results point in an exciting direction bringing discriminative and generative model evaluation closer. We will release code and benchmark setup soon.},
 abbr={NeurIPS 2023},
 selected={false},
 arxiv={2305.16397},
}


