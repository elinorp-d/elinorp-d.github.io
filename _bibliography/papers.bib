---
---
@inproceedings{pooledayan2024llmtargetedunderperformancedisproportionately,
      title={{LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users}}, 
      author={Elinor Poole-Dayan and Deb Roy and Jad Kabbara},
      year={2026},
      month = jan,
      eprint={2406.17737},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17737}, 
      abbr={AAAI 2026},
      booktitle = "Proceedings of the AAAI Conference on Artificial Intelligence",
      selected={true},
      address = "Singapore",
      arxiv={2406.17737},
      abstract = "While state-of-the-art large language models (LLMs) have shown impressive performance on many tasks, systematically evaluating undesirable behaviors of these models remains critical. In this work, we investigate how the quality of LLM responses changes in terms of information accuracy, truthfulness, and refusals depending on three user traits: English proficiency, education level, and country of origin. We present extensive experimentation on three state-of-the-art LLMs and two different datasets targeting truthfulness and factuality. Our findings suggest that undesirable behaviors in state-of-the-art LLMs occur disproportionately more for users with lower English proficiency, of lower education status, and originating from outside the US, rendering these models unreliable sources of information towards their most vulnerable users."
}

@inproceedings{hughes-etal-2025-computational,
    title = "Computational Analysis of Conversation Dynamics through Participant Responsivity",
    author = "Hughes, Margaret  and
      Roy, Brandon  and
      Poole-Dayan, Elinor  and
      Roy, Deb  and
      Kabbara, Jad",
    editor = "Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet",
    booktitle = "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.emnlp-main.1798/",
    pdf = "https://aclanthology.org/2025.emnlp-main.1798.pdf",
    pages = "35500--35519",
    ISBN = "979-8-89176-332-6",
    abbr={EMNLP 2025},
    selected={true},
    arxiv={2509.16464},
    abstract = "Growing literature explores toxicity and polarization in discourse, with comparatively less work on characterizing what makes dialogue prosocial and constructive. We explore conversational discourse and investigate a method for characterizing its quality built upon the notion of ``responsivity''{---}whether one person{'}s conversational turn is responding to a preceding turn. We develop and evaluate methods for quantifying responsivity{---}first through semantic similarity of speaker turns, and second by leveraging state-of-the-art large language models (LLMs) to identify the relation between two speaker turns. We evaluate both methods against a ground truth set of human-annotated conversations. Furthermore, selecting the better performing LLM-based approach, we characterize the nature of the response{---}whether it responded to that preceding turn in a substantive way or not. We view these responsivity links as a fundamental aspect of dialogue but note that conversations can exhibit significantly different responsivity structures. Accordingly, we then develop conversation-level derived metrics to address various aspects of conversational discourse. We use these derived metrics to explore other conversations and show that they support meaningful characterizations and differentiations across a diverse collection of conversations."
}
@misc{pooledayan2025aipoweredframeworkanalyzingcollective,
      title={An AI-Powered Framework for Analyzing Collective Idea Evolution in Deliberative Assemblies}, 
      author={Elinor Poole-Dayan and Deb Roy and Jad Kabbara},
      year={2025},
      month = sep,
      eprint={2509.12577},
      arxiv={2509.12577},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2509.12577}, 
      abbr={arXiv 2025},
      selected={false},
      abstract = "In an era of increasing societal fragmentation, political polarization, and erosion of public trust in institutions, representative deliberative assemblies are emerging as a promising democratic forum for developing effective policy outcomes on complex global issues. Despite theoretical attention, there remains limited empirical work that systematically traces how specific ideas evolve, are prioritized, or are discarded during deliberation to form policy recommendations. Addressing these gaps, this work poses two central questions: (1) How might we trace the evolution and distillation of ideas into concrete recommendations within deliberative assemblies? (2) How does the deliberative process shape delegate perspectives and influence voting dynamics over the course of the assembly? To address these questions, we develop LLM-based methodologies for empirically analyzing transcripts from a tech-enhanced in-person deliberative assembly. The framework identifies and visualizes the space of expressed suggestions. We also empirically reconstruct each delegate's evolving perspective throughout the assembly. Our methods contribute novel empirical insights into deliberative processes and demonstrate how LLMs can surface high-resolution dynamics otherwise invisible in traditional assembly outputs.",
}

@mastersthesis{poole-dayan2025dialogue,
  author       = {Poole-Dayan, Elinor},
  title        = {From Dialogue to Decision: An LLM-Powered Framework for Analyzing Collective Idea Evolution and Voting Dynamics in Deliberative Assemblies},
  school       = {Massachusetts Institute of Technology},
  year         = {2025},
  type         = {Master's thesis},
  address      = {Cambridge, MA},
  note         = {Advisor: Deb Roy},
  abbr         = {MIT 2025},
  url          = {https://www.media.mit.edu/publications/from-dialogue-to-decision-an-llm-powered-framework-deliberative-assemblies/},
  pdf          = {https://www.media.mit.edu/publications/from-dialogue-to-decision-an-llm-powered-framework-deliberative-assemblies/}
}

@inproceedings{fulay-etal-2024-relationship,
    title = "On the Relationship between Truth and Political Bias in Language Models",
    author = "Fulay, Suyash  and
      Brannon, William  and
      Mohanty, Shrestha  and
      Overney, Cassandra  and
      Poole-Dayan, Elinor  and
      Roy, Deb  and
      Kabbara, Jad",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.508",
    pdf = "https://aclanthology.org/2024.emnlp-main.508.pdf",
    pages = "9004--9018",
    abstract = "Language model alignment research often attempts to ensure that models are not only helpful and harmless, but also truthful and unbiased. However, optimizing these objectives simultaneously can obscure how improving one aspect might impact the others. In this work, we focus on analyzing the relationship between two concepts essential in both language model alignment and political science: truthfulness and political bias. We train reward models on various popular truthfulness datasets and subsequently evaluate their political bias. Our findings reveal that optimizing reward models for truthfulness on these datasets tends to result in a left-leaning political bias. We also find that existing open-source reward models (i.e., those trained on standard human preference datasets) already show a similar bias and that the bias is larger for larger models. These results raise important questions about the datasets used to represent truthfulness, potential limitations of aligning models to be both truthful and politically unbiased, and what language models capture about the relationship between truth and politics.",
    abbr={EMNLP 2024},
    selected={true},
    arxiv={2409.05283},
}

@inproceedings{NEURIPS2023_1a675d80,
 author = {Krojer, Benno and Poole-Dayan, Elinor and Voleti, Vikram and Pal, Chris and Reddy, Siva},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {8385--8405},
 publisher = {Curran Associates, Inc.},
 title = {Are Diffusion Models Vision-And-Language Reasoners?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/1a675d804f50509b8e21d0d3ca709d03-Paper-Conference.pdf},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/1a675d804f50509b8e21d0d3ca709d03-Paper-Conference.pdf},
 volume = {36},
 year = {2023},
 abstract = {Text-conditioned image generation models have recently shown immense qualitative success using denoising diffusion processes. However, unlike discriminative vision-and-language models, it is a non-trivial task to subject these diffusion-based generative models to automatic fine-grained quantitative evaluation of high-level phenomena such as compositionality.Towards this goal, we perform two innovations. First, we transform diffusion-based models (in our case, Stable Diffusion) for any image-text matching (ITM) task using a novel method called DiffusionITM.Second, we introduce the Generative-Discriminative Evaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language tasks, bias evaluation and detailed analysis.We find that Stable Diffusion + DiffusionITM is competitive on many tasks and outperforms CLIP on compositional tasks like like CLEVR and Winoground.We further boost its compositional performance with a transfer setup by fine-tuning on MS-COCO while retaining generative capabilities. We also measure the stereotypical bias in diffusion models, and find that Stable Diffusion 2.1 is, for the most part, less biased than Stable Diffusion 1.5.Overall, our results point in an exciting direction bringing discriminative and generative model evaluation closer. We will release code and benchmark setup soon.},
 abbr={NeurIPS 2023},
 selected={false},
 arxiv={2305.16397},
}

@inproceedings{meade-etal-2022-empirical,
    title = "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
    author = "Meade, Nicholas  and
      Poole-Dayan, Elinor  and
      Reddy, Siva",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.132",
    pdf = "https://aclanthology.org/2022.acl-long.132.pdf",
    doi = "10.18653/v1/2022.acl-long.132",
    pages = "1878--1898",
    abstract = "Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on. This has attracted attention to developing techniques that mitigate such biases. In this work, we perform an empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model{'}s language modeling ability, as well as its performance on downstream NLU tasks. We experimentally find that: (1) Self-Debias is the strongest debiasing technique, obtaining improved scores on all bias benchmarks; (2) Current debiasing techniques perform less consistently when mitigating non-gender biases; And (3) improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability, making it difficult to determine whether the bias mitigation was effective.",
    abbr={ACL 2022},
    arxiv={2110.08527},
    selected={true},
}


